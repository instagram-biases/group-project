# **DECODING BIASES IN ARTIFICIAL INTELLIGENCE - FINAL PAPER**

# **INTRODUCTION**
In 2019, the BBC ran a story about a 12-year-old girl who, having barely joined Instagram, was instantly bombarded with ads for beauty and health products. It turns out that she was actually much more interested in athletic and academic pursuits, and had joined the platform to follow the accounts of her sports idols. As a society, we are increasingly becoming aware of the dangers of sexist stereotypes. We are also aware of the immense harm that these kinds of imagery have on self-confidence and body image. 

As we read this story, we realized just how big the amount of information that Instagram has on us is, and how it influences the advertisements sent to our feed. We therefore decided to study the biases in Instagram’s ad selection process. We increasingly realized that targeted advertising can be very harmful, and that we might have interests that we did not explicitly nor publicly express, but that are figured out in ways algorithms are aware of. 


![Alt text](relative/path/to/Test.jpg?raw=true "Test")



# **HYPOTHESIS**
We have conjectured that Instagram’s targeted ads are based on information that we are not explicitly aware is being used. 

The goal of our study is to take a user-focused approach to analyse and understand what data is used to generate potentially gendered-biased Instagram ads, and use our results to inform and open the debate on these covert practices. 

_Note of the authors: this study deliberately relies on the “classical” vision of gender, as a bi-categorical variable. Our rationale is twofold: first, Instagram only allows for two such gender categories when signing up for an account (in addition to the gender-neutral category, which we also used). Second, given the rather limited scale of our research project, it is both easier and more accurate to focus on the male/female divide.
_


***********************



# **LITERATURE REVIEW**
As academia starts to catch up with big tech companies’ covert efforts to amass the biggest amount of data possible on its users, the literature on biases in ad algorithms is growing strong. One of the main (and most mediatized) issues regarding social media advertising is obviously the issue of political ads. Digital advertising on social media platforms is hugely popular due to how they empower the client’s ad to target messages directed at users with a never-seen-before accuracy, including inferences about the user’s political affiliations (Ali, 2019) (Heilweil, 2020). Prior work has shown that platforms’ ad algorithms can selectively choose to deliver ads within the intended target audiences in ways that more often than not lead to demographic skews. These skews always go along gender lines, often without the advertiser’s nor the public’s knowledge (Brown et al., 2018).

When it comes to the content of the ads themselves, examples like the aforementioned case of the 12-year old British girl are numerous and deserve our attention. Facebook Inc., Instagram’s parent company, disproportionately shows certain types of job advertisements to women and men, calling into question the company’s alleged efforts to root out biases in its algorithms (Jeff Horwitz, 2021). A 2018 Harvard study showed that, despite explicit intentions by the ad-producer to be gender neutral, a Facebook/Instagram advertisement for STEM (Science, Technology, Engineering, Math) careers had been shown much more often to men than women on the platform (Lambrecht, Anja, Catherine Tucker, 2018).  

Women continue to be sexualized, stereotyped, and under-represented in online ads when compared to men (Jen Gennai, 2020). There is clear evidence that these portrayals have consequential real-world effects on the way women behave and are treated. This is why we felt the need to perform this study, as it not only concerns Instagram users, but society as a whole. 

“One of our top priorities is protecting people from discrimination” said Sheryl Sandberg, Facebook’s Chief Operating Officer, in 2019. This statement came after Facebook had just launched a new, supposedly unbiased algorithm to display ads on its multiple platforms. As academia has proven (Ava Kofman, Ariana Tobin, 2019), as well as we are about to find out in our study, these efforts seem not to be working as well as the executives at Facebook claim. 




***********************



# **METHODOLOGY**
Our study uses the so-called “netnographic approach” (as theorized by Costello et al., 2017). Netnography is a data collection technique which is ‘a qualitative research methodology that adapts ethnographic research techniques through electronic networks’. Netnography consists of a thorough analysis of data in a natural setting and without the intervention of the researcher (Bailey, 2007). It is an unobtrusive way of gaining information, especially useful to gather data on topics such as gender issues (Gilchrist, Ravenscroft, et al., 2011). We will follow the  netnographic approach to conduct the analyses as part of our research. 

_What Instagram tells its users_
Whenever a user is sent an ad on their Instagram feed, a small icon on the add allows the user to get some information about the add, and why they see this particular ad. Here’s what Instagram tells us: 
How does Instagram decide which ads to show me? We want to show you ads from businesses that are relevant to you, and to do that we use information about what you do on Instagram as well as your activity on third-party sites and apps you use. We also may use information provided by businesses outside of Instagram or Facebook Company Products to decide which ads to show you. For example, you might see ads based on the people you follow and posts you like on Instagram, your information, the websites and apps you visit, or information advertisers, their partners, and our marketing partners share with us that they already have, like your email address.

_The anonymization process to control for “data contamination”_
As soon as we read this informative message, we quickly realized how difficult it would be to create a perfectly virgin instagram account, devoid of any prior data, in order to avoid what we’ve called a “contamination” of our experiment (meaning when information that is not explicitly collected by Instagram is used to steer our ads in a certain direction). 

After a painstaking process of trial and error, we came up with what we believe is the most secure and safe way of creating an account without any sort of   contamination (or with as little contamination as possible). 

First, we employed a single computer to create the email addresses that would later be used to open our experiment’s Instagram accounts.
Second, we turned on a secure VPN connection, to make sure that our IP address would be untraceable. As such, neither the email provider, nor Instagram, could trace the email addresses back to our computer and our browsing histories. 
Third, we downloaded a new browser without any browsing history, namely Mozilla Firefox, and solely navigated using private navigation. 
Fourth, we created twenty (20) email addresses through ProtonMail, the safest and most secure email provider according to professionals. Each email address was created following a rationalized identical format: “saxo.phone[XX]@protonmail.com”, where the “X” is a given number. The use of “saxo.phone” is meant to throw off any attempt by Instagram to identify the user’s gender via their email (i.e. Instagram could very well identify a “john.deer@[...]” as a male. 

Following this thorough (some might say almost paranoid) process, we had thus created a perfectly hermetic and gender-neutral environment that we then used to create our twenty (20) experimental Instagram accounts. The problem with Instagram is that, despite the EU’s GDPR, you cannot access the platform without explicitly consenting to cookie collections. This is why we painstakingly continued working from a single computer, on a single virgin browser, in private navigation mode, with a VPN activated (and located in Paris, France, in order only to receive French ads that we would know and could easily categorize).

Creating the Instagram accounts for our experiment was not an issue (albeit a little time-consuming): we simply used the email addresses that we had created and signed up to create the accounts. We used gender-neutral aliases that would control for any sort of analysis by Instagram’s algorithms. The name given to our Instagram profile is also gender-neutral, allowing us to control for this variable. 

***********************

# **PROTOCOL**
After yet another long brainstorming session, we came up with a strong, reliable protocol to test our hypothesis. Our process is split between two distinct waves of experimentation, as well as two distinct categories for classifying the ads that our profiles are sent.

_The first wave_
* For the first wave of the experiment, we decide to go with the following six types of profiles: 
* Self-reported Female profiles, liking “male” content
* Self-reported Female profiles, liking “female” content 
* Self-reported Male profiles, liking “male” content
* Self-reported Male profiles, liking “female” content
* Self-reported Gender-neutral profiles, liking “male” content 
* Self-reported Gender-neutral profiles, liking “female” content 

The idea is to verify how much Instagram relies on the self-declaration of the user’s gender to orientate advertisements. In other words, if a female profile likes typical male content, will the user still receive typical female ads, or will the pages that the user follows define the ads that she receives? 

_The seconde wave_
The second wave of experimentation over Instagram’s advertisement strategy uses a broader and more complex set of user profiles:
* Female profiles, with a “female profile picture”, liking “female” content
* Female profiles, with a “female profile picture”, liking “female” content
* Female profiles, with a “male profile picture”, liking “female” content
* Female profiles, with a “male profile picture”, liking “male” content
* Female profiles, with a “gender-neutral profile picture”, liking “male” content
* Female profiles, with a “gender-neutral profile picture”, liking “female” content

* Male profiles, with a “female profile picture”, liking “female” content
* Male profiles, with a “female profile picture”, liking “female” content
* Male profiles, with a “male profile picture”, liking “female” content
* Male profiles, with a “male profile picture”, liking “male” content
* Male profiles, with a “gender-neutral profile picture”, liking “male” content
* Male profiles, with a “gender-neutral profile picture”, liking “female” content

* Gender-neutral profiles, with a “female profile picture”, liking “female” content
* Gender-neutral profiles, with a “female profile picture”, liking “female” content
* Gender-neutral profiles, with a “male profile picture”, liking “female” content
* Gender-neutral profiles, with a “male profile picture”, liking “male” content
* Gender-neutral profiles, with a “gender-neutral profile picture”, liking “male” content
* Gender-neutral profiles, with a “gender-neutral profile picture”, liking “female” content

The idea is to verify whether Instagram relies on the profile pictures of the users, when sending ads to their feed, in addition to using the liked content.
The ad categories


***********************

# **RESULTS** 
Caractériser contenu masculin et contenu féminin

_Challenges_
	It must be noted that our experiment is only as valid as our ad categories. Indeed, although we tried to establish our two categories (“typically male”/”typically female”) as scientifically as possible, our own judgement and personal biases have played a role in defining which ad would be classified as one or the other category. By using a unanimous vote amongst the three of us to classify all the ads, we tried to neutralize our own biases, but we are aware that we might have committed mistakes. 

***********************

# **BIBLIOGRAPHY**
_Brown, T. W. et al., Exposure to opposing views on social media can increase political polarization, National Academy of Sciences of the USA (2018)_

_Muhammad Ali et al., Ad Delivery Algorithms: The Hidden Arbiters of Political Messaging, Cornell University Press (2019)_

_Muhammad Ali, Discrimination through optimization: How Facebook’s ad delivery can lead to skewed outcomes, Cornell University Press (2019)_

_Jen Gennai, Meghana Bhatt, Take back the Ad: Understanding the impact of female-sexualized ads on male and female viewers (2020)_

_Jeff Horwitz, Facebook Algorithm Shows Gender Bias in Job Ads, Wall Street Journal (2020)_

_Rebecca Heilweil, Facebook glitch blocks certain political ads, raising new questions about transparency (2020)_

_Lambrecht, Anja, Catherine Tucker, An Empirical Study into Apparent Gender-Based Discrimination in the Display of STEM Career Ads, Harvard Kennedy School (2018)_

_Ava Kofman, Ariana Tobin, Facebook Ads Can Still Discriminate Against Women, Despite a Civil Rights Settlement, ProPublica (2019)_

_Vox News, Facebook showed this ad to 95% women. Is that a problem?, link, (2020)_

_Judith Duportail, Nicolas Kayser-Bril, Edouard Richard, Kira Schacht, The skin bias in Instagram, AlgorithmWatch (2020)__
